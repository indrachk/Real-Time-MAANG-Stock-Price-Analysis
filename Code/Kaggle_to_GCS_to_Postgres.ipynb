{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\18572\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: google-cloud-storage in c:\\users\\18572\\anaconda3\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.6.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.23.3 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.23.4)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.14.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.28.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.61.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (4.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (5.3.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.11)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: gcsfs in c:\\users\\18572\\anaconda3\\lib\\site-packages (2023.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (1.1.0)\n",
      "Requirement already satisfied: google-auth>=1.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (2.23.4)\n",
      "Requirement already satisfied: fsspec==2023.10.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (2.28.1)\n",
      "Requirement already satisfied: decorator>4.1.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (4.4.2)\n",
      "Requirement already satisfied: google-cloud-storage in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (2.13.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (3.9.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.9.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth>=1.2->gcsfs) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage->gcsfs) (1.5.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage->gcsfs) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage->gcsfs) (2.6.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage->gcsfs) (2.14.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->gcsfs) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->gcsfs) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->gcsfs) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->gcsfs) (2.0.4)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (4.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.61.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\18572\\anaconda3\\lib\\site-packages (1.4.39)\n",
      "Requirement already satisfied: pandas in c:\\users\\18572\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from sqlalchemy) (1.1.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\18572\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: kaggle in c:\\users\\18572\\anaconda3\\lib\\site-packages (1.5.16)\n",
      "Requirement already satisfied: certifi in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (2022.9.14)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (4.64.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (1.26.11)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\18572\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\18572\\anaconda3\\lib\\site-packages (from bleach->kaggle) (21.3)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\18572\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from packaging->bleach->kaggle) (3.0.9)\n",
      "Requirement already satisfied: google-cloud-secret-manager in c:\\users\\18572\\anaconda3\\lib\\site-packages (2.16.4)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-secret-manager) (2.14.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-secret-manager) (1.22.3)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-secret-manager) (0.12.7)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-secret-manager) (4.25.1)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (2.28.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (1.61.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (2.23.4)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (1.59.3)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (1.59.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (5.3.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (0.2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (1.26.11)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas --upgrade\n",
    "!pip install google-cloud-storage\n",
    "!pip install gcsfs\n",
    "!pip install sqlalchemy pandas psycopg2\n",
    "!pip install python-dotenv\n",
    "!pip install kaggle --upgrade\n",
    "!pip install google-cloud-secret-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "from datetime import date\n",
    "register_adapter(np.int64, AsIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly blobs:  [<Blob: dwdi-de-project, testdatasets/AMAZON_monthly.csv, 1701308551221295>, <Blob: dwdi-de-project, testdatasets/APPLE_monthly.csv, 1701308553671219>, <Blob: dwdi-de-project, testdatasets/GOOGLE_monthly.csv, 1701308555877880>, <Blob: dwdi-de-project, testdatasets/META_monthly.csv, 1701308557770466>, <Blob: dwdi-de-project, testdatasets/NETFLIX_monthly.csv, 1701308559722407>]\n",
      "result: \n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "            Date        Open        High         Low       Close   Adj Close  \\\n",
      "1402  2023-02-01  353.859985  379.429993  314.299988  322.130005  322.130005   \n",
      "1403  2023-03-01  321.549988  345.839996  285.329987  345.480011  345.480011   \n",
      "1404  2023-04-01  341.829987  349.799988  316.100006  329.929993  329.929993   \n",
      "1405  2023-05-01  329.440002  405.109985  315.619995  395.230011  395.230011   \n",
      "1406  2023-06-01  397.410004  448.649994  393.079987  440.489990  440.489990   \n",
      "1407  2023-07-01  439.760010  485.000000  411.880005  438.970001  438.970001   \n",
      "1408  2023-08-01  437.369995  445.250000  398.149994  433.679993  433.679993   \n",
      "1409  2023-09-01  437.730011  453.450012  371.100006  377.600006  377.600006   \n",
      "1410  2023-10-01  377.480011  418.839996  344.730011  411.690002  411.690002   \n",
      "1411  2023-11-01  414.769989  482.700012  414.179993  479.170013  479.170013   \n",
      "\n",
      "         Volume  company  \n",
      "1402  123374100  NETFLIX  \n",
      "1403  157555400  NETFLIX  \n",
      "1404  127984500  NETFLIX  \n",
      "1405  145471700  NETFLIX  \n",
      "1406  147230800  NETFLIX  \n",
      "1407  168720200  NETFLIX  \n",
      "1408  107298900  NETFLIX  \n",
      "1409  100278600  NETFLIX  \n",
      "1410  164021900  NETFLIX  \n",
      "1411   61706300  NETFLIX  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_17260\\2255927682.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Dataset downloaded and processed successfully.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set key credentials file path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:/Disk_D/Course Work/Data Warehousing/Project-2/Keys/alien-grove-405422-bb57fda72219.json'\n",
    "# Set your Kaggle API key\n",
    "api = KaggleApi()\n",
    "# api.CONFIG_NAME_USER='saivarunkumarnamburi'\n",
    "# api.CONFIG_NAME_KEY='4e18a51c732b29ec81207232f7381392'\n",
    "# api.read_config_environment({\"username\":\"saivarunkumarnamburi\",\"key\":\"4e18a51c732b29ec81207232f7381392\"})\n",
    "api.authenticate()\n",
    "\n",
    "def connect_db_bulk(df,tbname):\n",
    "    #Get Credentials\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "    # Use the variables in your database connection logic\n",
    "    connection_params = {\n",
    "        'host': db_host,\n",
    "        'port': db_port,\n",
    "        'database': db_name,\n",
    "        'user': db_user,\n",
    "        'password': db_password,\n",
    "    }\n",
    "    # Establish a connection to your PostgreSQL database\n",
    "    connection = psycopg2.connect(**connection_params)\n",
    "\n",
    "    # Create a cursor to execute SQL statements\n",
    "    cursor = connection.cursor()\n",
    "    table_name = tbname\n",
    "    schema_name = 'public'\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    max_dates_by_company = df.groupby('company')['date'].max()\n",
    "    # max_dates_by_company_df = max_dates_by_company.reset_index()\n",
    "    max_dates_by_company = max_dates_by_company.reset_index().sort_values(by='company')\n",
    "    # print(max_dates_by_company)\n",
    "    sql_query = \"SELECT company, MAX(date) as date FROM public.testmonthly GROUP BY company ORDER BY company;\"\n",
    "    # Execute the query and store the result in a DataFrame\n",
    "    db_result = pd.read_sql_query(sql_query, connection)\n",
    "    db_result['date'] = pd.to_datetime(db_result['date']).dt.date\n",
    "    res_cd = pd.DataFrame()\n",
    "\n",
    "    if len(db_result) == 0:\n",
    "        # res_cd = max_dates_by_company\n",
    "        insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "        records = df.to_records(index=False)\n",
    "        values = [tuple(record) for record in records]\n",
    "        cursor.executemany(insert_query, values)\n",
    "    else:\n",
    "        db_result['date'] = pd.to_datetime(db_result['date']).dt.date\n",
    "        for i in range(len(db_result)):\n",
    "            if db_result.loc[i, \"date\"] < max_dates_by_company.loc[i, \"date\"]:\n",
    "                res_cd = res_cd._append(db_result.loc[i])\n",
    "        # print(\"Res Cd:\", res_cd)\n",
    "        \n",
    "        res = pd.DataFrame()\n",
    "        for i in range(len(res_cd)):\n",
    "            company_res = df[(df['company'] == res_cd.loc[i, 'company']) & (df['date'] > res_cd.loc[i,'date'])]\n",
    "            res = res._append(company_res)\n",
    "    \n",
    "        print(\"result: \\n\",res)\n",
    "        if(len(res)!=0):\n",
    "            insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "            records = res.to_records(index=False)\n",
    "            values = [tuple(record) for record in records]\n",
    "            cursor.executemany(insert_query, values)\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def connect_db_bulk_parallel(df):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunk_size = 5000  # Adjust the chunk size as needed\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system capacity\n",
    "        # Submit each chunk for parallel processing\n",
    "        futures = [executor.submit(connect_db_bulk, chunk,'testmonthly') for chunk in chunks]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def download_dataset(request):\n",
    "    # Specify the dataset to download\n",
    "    dataset_name = \"nikhil1e9/netflix-stock-price\"\n",
    "\n",
    "    # Set the Cloud Storage bucket name\n",
    "    bucket_name = \"dwdi-de-project\"\n",
    "\n",
    "    # Specify the Cloud Storage directory to store downloaded files\n",
    "    storage_directory = \"testdatasets/\"\n",
    "\n",
    "    # Download the dataset from Kaggle\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        api.dataset_download_files(dataset=dataset_name, path=temp_dir, unzip=True)\n",
    "\n",
    "        # List downloaded files\n",
    "        downloaded_files = os.listdir(temp_dir)\n",
    "        # Upload each file to Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        for file_name in downloaded_files:\n",
    "            local_path = os.path.join(temp_dir, file_name)\n",
    "            storage_path = os.path.join(storage_directory, file_name)\n",
    "\n",
    "            # Upload the file to Cloud Storage\n",
    "            blob = bucket.blob(storage_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "\n",
    "        # Process the uploaded files\n",
    "        process_uploaded_files(bucket_name, storage_directory)\n",
    "\n",
    "    return \"Dataset downloaded and processed successfully.\"\n",
    "\n",
    "def delete_objects(bucket_name, prefix):\n",
    "    # Create a Storage client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List all objects with the specified prefix\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    # Delete each object\n",
    "    for blob in blobs:\n",
    "        blob.delete()\n",
    "\n",
    "def process_uploaded_files(bucket_name, storage_directory):\n",
    "    data_dict = {\"monthly\": []}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List files in the Cloud Storage directory\n",
    "    blobs = list(bucket.list_blobs(prefix=storage_directory))\n",
    "    monthly_blobs = [blob for blob in blobs if 'monthly' in blob.name]\n",
    "    print(\"Monthly blobs: \", monthly_blobs)\n",
    "    for blob in monthly_blobs:\n",
    "        # Process only CSV files\n",
    "        if blob.name.endswith('.csv'):\n",
    "            # Download the file content\n",
    "            content = blob.download_as_text()\n",
    "            # Process the content (example: convert to DataFrame)\n",
    "            df = pd.read_csv(StringIO(content))\n",
    "            # Example: Append the DataFrame to the 'monthly' list\n",
    "            company=blob.name.split('_')[0]\n",
    "            company_updated = company.split('/')[1]\n",
    "            data_dict[\"monthly\"].append(df.assign(company=company_updated))\n",
    "\n",
    "    delete_objects(bucket_name,storage_directory)\n",
    "    # Concatenate DataFrames\n",
    "    monthly_data = pd.concat(data_dict[\"monthly\"], ignore_index=True)\n",
    "    column_mapping = {\n",
    "    'Date': 'date',\n",
    "    'Open': 'open',\n",
    "    'High': 'high',\n",
    "    'Low': 'low',\n",
    "    'Close': 'close',\n",
    "    'Adj Close': 'adj_close',\n",
    "    'Volume': 'volumn',\n",
    "    'Company': 'company'\n",
    "    }\n",
    "    # Map DataFrame columns to PostgreSQL columns\n",
    "    df_monthly_mapped = monthly_data.rename(columns=column_mapping)\n",
    "    connect_db_bulk_parallel(df_monthly_mapped)\n",
    "    print(monthly_data.tail(10))\n",
    "\n",
    "download_dataset(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly blobs:  [<Blob: dwdi-de-project, testdatasets/AMAZON_weekly.csv, 1701308752306413>, <Blob: dwdi-de-project, testdatasets/APPLE_weekly.csv, 1701308754670846>, <Blob: dwdi-de-project, testdatasets/GOOGLE_weekly.csv, 1701308756698798>, <Blob: dwdi-de-project, testdatasets/META_weekly.csv, 1701308758960724>, <Blob: dwdi-de-project, testdatasets/NETFLIX_weekly.csv, 1701308760830063>]\n",
      "Main df dates \n",
      "    index  company        date\n",
      "0      0   AMAZON  2023-11-27\n",
      "1      1    APPLE  2023-11-27\n",
      "2      2   GOOGLE  2023-11-27\n",
      "3      3     META  2023-11-27\n",
      "4      4  NETFLIX  2023-11-27\n",
      "Database Result \n",
      "    company        date\n",
      "0   AMAZON  2023-11-13\n",
      "1    APPLE  2023-11-13\n",
      "2   GOOGLE  2023-11-13\n",
      "3     META  2023-11-13\n",
      "4  NETFLIX  2023-11-13\n",
      "result: \n",
      "             date        open        high         low       close   adj_close  \\\n",
      "1384  2023-11-20  145.130005  147.740005  141.500000  146.740005  146.740005   \n",
      "1385  2023-11-27  147.529999  149.259995  146.880005  147.729996  147.729996   \n",
      "3627  2023-11-20  189.889999  192.929993  189.250000  189.970001  189.970001   \n",
      "3628  2023-11-27  189.919998  190.669998  188.899994  189.789993  189.789993   \n",
      "4634  2023-11-20  135.500000  141.100006  135.490005  138.220001  138.220001   \n",
      "4635  2023-11-27  137.570007  139.630005  137.539993  138.050003  138.050003   \n",
      "5237  2023-11-20  334.890015  342.920013  334.190002  338.230011  338.230011   \n",
      "5238  2023-11-27  336.179993  339.899994  334.200012  334.700012  334.700012   \n",
      "6361  2023-11-20  465.399994  482.700012  465.399994  479.559998  479.559998   \n",
      "6362  2023-11-27  479.029999  482.000000  475.350006  479.170013  479.170013   \n",
      "\n",
      "         volumn  company  \n",
      "1384  181224700   AMAZON  \n",
      "1385   53666700   AMAZON  \n",
      "3627  148305600    APPLE  \n",
      "3628   40500500    APPLE  \n",
      "4634   63352500   GOOGLE  \n",
      "4635   17868000   GOOGLE  \n",
      "5237   45158600     META  \n",
      "5238   15646300     META  \n",
      "6361   10861600  NETFLIX  \n",
      "6362    3623800  NETFLIX  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_17260\\3682805549.py:49: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Dataset downloaded and processed successfully.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set key credentials file path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:/Disk_D/Course Work/Data Warehousing/Project-2/Keys/alien-grove-405422-bb57fda72219.json'\n",
    "# Set your Kaggle API key\n",
    "api = KaggleApi()\n",
    "# api.CONFIG_NAME_USER='saivarunkumarnamburi'\n",
    "# api.CONFIG_NAME_KEY='4e18a51c732b29ec81207232f7381392'\n",
    "# api.read_config_environment({\"username\":\"saivarunkumarnamburi\",\"key\":\"4e18a51c732b29ec81207232f7381392\"})\n",
    "api.authenticate()\n",
    "\n",
    "def connect_db_bulk(df,tbname):\n",
    "    #Get Credentials\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "    # Use the variables in your database connection logic\n",
    "    connection_params = {\n",
    "        'host': db_host,\n",
    "        'port': db_port,\n",
    "        'database': db_name,\n",
    "        'user': db_user,\n",
    "        'password': db_password,\n",
    "    }\n",
    "    # Establish a connection to your PostgreSQL database\n",
    "    connection = psycopg2.connect(**connection_params)\n",
    "\n",
    "    # Create a cursor to execute SQL statements\n",
    "    cursor = connection.cursor()\n",
    "    table_name = tbname\n",
    "    schema_name = 'public'\n",
    "    # df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    # Convert 'date' to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    # Drop rows with missing dates\n",
    "    df = df.dropna(subset=['date'])\n",
    "    # Extract date component\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "    max_dates_by_company_df = df.groupby('company')['date'].max()\n",
    "    max_dates_by_company_df = max_dates_by_company_df.reset_index()\n",
    "    max_dates_by_company = max_dates_by_company_df.reset_index().sort_values(by='company')\n",
    "    # max_dates_by_company_df = df.groupby('company')['date'].max().reset_index()\n",
    "    # max_dates_by_company = max_dates_by_company_df.sort_values(by='company')\n",
    "    print(\"Main df dates \\n\",max_dates_by_company)\n",
    "    sql_query = \"SELECT company, MAX(date) as date FROM public.weekly GROUP BY company ORDER BY company;\"\n",
    "    # Execute the query and store the result in a DataFrame\n",
    "    db_result = pd.read_sql_query(sql_query, connection)\n",
    "    db_result['date'] = pd.to_datetime(db_result['date']).dt.date\n",
    "    res_cd = pd.DataFrame()\n",
    "\n",
    "    if len(db_result) == 0:\n",
    "        # res_cd = max_dates_by_company\n",
    "        insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "        records = df.to_records(index=False)\n",
    "        values = [tuple(record) for record in records]\n",
    "        cursor.executemany(insert_query, values)\n",
    "    else:\n",
    "        db_result['date'] = pd.to_datetime(db_result['date']).dt.date\n",
    "        print(\"Database Result \\n\",db_result)\n",
    "        for i in range(len(db_result)):\n",
    "            if db_result.iloc[i][\"date\"] < max_dates_by_company.loc[i, \"date\"]:\n",
    "                res_cd = res_cd._append(db_result.loc[i])\n",
    "        # print(\"Res Cd:\", res_cd)\n",
    "        \n",
    "        res = pd.DataFrame()\n",
    "        for i in range(len(res_cd)):\n",
    "            company_res = df[(df['company'] == res_cd.loc[i, 'company']) & (df['date'] > res_cd.loc[i,'date'])]\n",
    "            res = res._append(company_res)\n",
    "    \n",
    "        print(\"result: \\n\",res)\n",
    "        if(len(res)!=0):\n",
    "            insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "            records = res.to_records(index=False)\n",
    "            values = [tuple(record) for record in records]\n",
    "            cursor.executemany(insert_query, values)\n",
    "\n",
    "    # connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def connect_db_bulk_parallel(df):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunk_size = 5000  # Adjust the chunk size as needed\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system capacity\n",
    "        # Submit each chunk for parallel processing\n",
    "        futures = [executor.submit(connect_db_bulk, chunk,'weekly') for chunk in chunks]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def download_dataset(request):\n",
    "    # Specify the dataset to download\n",
    "    dataset_name = \"nikhil1e9/netflix-stock-price\"\n",
    "\n",
    "    # Set the Cloud Storage bucket name\n",
    "    bucket_name = \"dwdi-de-project\"\n",
    "\n",
    "    # Specify the Cloud Storage directory to store downloaded files\n",
    "    storage_directory = \"testdatasets/\"\n",
    "\n",
    "    # Download the dataset from Kaggle\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        api.dataset_download_files(dataset=dataset_name, path=temp_dir, unzip=True)\n",
    "\n",
    "        # List downloaded files\n",
    "        downloaded_files = os.listdir(temp_dir)\n",
    "        # Upload each file to Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        for file_name in downloaded_files:\n",
    "            local_path = os.path.join(temp_dir, file_name)\n",
    "            storage_path = os.path.join(storage_directory, file_name)\n",
    "\n",
    "            # Upload the file to Cloud Storage\n",
    "            blob = bucket.blob(storage_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "\n",
    "        # Process the uploaded files\n",
    "        process_uploaded_files(bucket_name, storage_directory)\n",
    "\n",
    "    return \"Dataset downloaded and processed successfully.\"\n",
    "\n",
    "def delete_objects(bucket_name, prefix):\n",
    "    # Create a Storage client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List all objects with the specified prefix\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    # Delete each object\n",
    "    for blob in blobs:\n",
    "        blob.delete()\n",
    "\n",
    "def process_uploaded_files(bucket_name, storage_directory):\n",
    "    data_dict = {\"weekly\": []}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List files in the Cloud Storage directory\n",
    "    blobs = list(bucket.list_blobs(prefix=storage_directory))\n",
    "    weekly_blobs = [blob for blob in blobs if 'weekly' in blob.name]\n",
    "    print(\"Weekly blobs: \", weekly_blobs)\n",
    "    for blob in weekly_blobs:\n",
    "        # Process only CSV files\n",
    "        if blob.name.endswith('.csv'):\n",
    "            # Download the file content\n",
    "            content = blob.download_as_text()\n",
    "            # Process the content (example: convert to DataFrame)\n",
    "            df = pd.read_csv(StringIO(content))\n",
    "            # Example: Append the DataFrame to the 'monthly' list\n",
    "            company=blob.name.split('_')[0]\n",
    "            company_updated = company.split('/')[1]\n",
    "            data_dict[\"weekly\"].append(df.assign(company=company_updated))\n",
    "\n",
    "    delete_objects(bucket_name,storage_directory)\n",
    "    # Concatenate DataFrames\n",
    "    weekly_data = pd.concat(data_dict[\"weekly\"], ignore_index=True)\n",
    "    column_mapping = {\n",
    "    'Date': 'date',\n",
    "    'Open': 'open',\n",
    "    'High': 'high',\n",
    "    'Low': 'low',\n",
    "    'Close': 'close',\n",
    "    'Adj Close': 'adj_close',\n",
    "    'Volume': 'volumn',\n",
    "    'Company': 'company'\n",
    "    }\n",
    "    # Map DataFrame columns to PostgreSQL columns\n",
    "    df_weekly_mapped = weekly_data.rename(columns=column_mapping)\n",
    "    # print(weekly_data.tail(10))\n",
    "    connect_db_bulk(df_weekly_mapped,'weekly')\n",
    "    \n",
    "\n",
    "download_dataset(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
