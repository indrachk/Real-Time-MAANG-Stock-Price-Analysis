{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in c:\\users\\18572\\anaconda3\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.23.3 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.23.4)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.6.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.28.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.14.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.61.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (4.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (5.3.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2022.9.14)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: gcsfs in c:\\users\\18572\\anaconda3\\lib\\site-packages (2023.10.0)\n",
      "Requirement already satisfied: google-auth>=1.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (2.23.4)\n",
      "Requirement already satisfied: decorator>4.1.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (4.4.2)\n",
      "Requirement already satisfied: google-auth-oauthlib in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (1.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-storage in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (2.13.0)\n",
      "Requirement already satisfied: fsspec==2023.10.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from gcsfs) (3.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.9.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth>=1.2->gcsfs) (5.3.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage->gcsfs) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage->gcsfs) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage->gcsfs) (2.6.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-cloud-storage->gcsfs) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->gcsfs) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->gcsfs) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->gcsfs) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->gcsfs) (2.0.4)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (4.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.61.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\18572\\anaconda3\\lib\\site-packages (1.4.39)\n",
      "Requirement already satisfied: pandas in c:\\users\\18572\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from sqlalchemy) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\18572\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.16.tar.gz (83 kB)\n",
      "     ---------------------------------------- 83.6/83.6 kB 2.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (2022.9.14)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (4.64.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (1.26.11)\n",
      "Requirement already satisfied: bleach in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\18572\\anaconda3\\lib\\site-packages (from bleach->kaggle) (21.3)\n",
      "Requirement already satisfied: webencodings in c:\\users\\18572\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\18572\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from packaging->bleach->kaggle) (3.0.9)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.16-py3-none-any.whl size=110686 sha256=09cce12812b0e14374c05c83f388565c4b838e396e7b5fea85c11b1044cb5dca\n",
      "  Stored in directory: c:\\users\\18572\\appdata\\local\\pip\\cache\\wheels\\d2\\ed\\a5\\da3a0cfb13373d1ace41cafa4f2467d858c55c52473ba72799\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.5.16\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-storage\n",
    "!pip install gcsfs\n",
    "!pip install sqlalchemy pandas psycopg2\n",
    "!pip install python-dotenv\n",
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      open      high       low     close  adj_close      volumn  \\\n",
      "0  1997-06-01  0.075521  0.085417  0.068750  0.077083   0.077083  1063272000   \n",
      "1  1997-07-01  0.077083  0.128646  0.075521  0.119792   0.119792  2172288000   \n",
      "2  1997-08-01  0.117188  0.120833  0.096875  0.116927   0.116927   638136000   \n",
      "3  1997-09-01  0.117188  0.240625  0.115625  0.216927   0.216927  2157240000   \n",
      "4  1997-10-01  0.221875  0.275000  0.176042  0.254167   0.254167  2103744000   \n",
      "\n",
      "  company  \n",
      "0  AMAZON  \n",
      "1  AMAZON  \n",
      "2  AMAZON  \n",
      "3  AMAZON  \n",
      "4  AMAZON  \n"
     ]
    }
   ],
   "source": [
    "# Monthly Data Ingestion\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "register_adapter(np.int64, AsIs)\n",
    "\n",
    "# set key credentials file path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:/Disk_D/Course Work/Data Warehousing/Project-2/Keys/alien-grove-405422-bb57fda72219.json'\n",
    "\n",
    "def connect_db_bulk(df,tbname):\n",
    "    #Get Credentials\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "    # Use the variables in your database connection logic\n",
    "    connection_params = {\n",
    "        'host': db_host,\n",
    "        'port': db_port,\n",
    "        'database': db_name,\n",
    "        'user': db_user,\n",
    "        'password': db_password,\n",
    "    }\n",
    "    # Establish a connection to your PostgreSQL database\n",
    "    connection = psycopg2.connect(**connection_params)\n",
    "\n",
    "    # Create a cursor to execute SQL statements\n",
    "    cursor = connection.cursor()\n",
    "    table_name = tbname\n",
    "    schema_name = 'public'\n",
    "    \n",
    "    insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "    records = df.to_records(index=False)\n",
    "    values = [tuple(record) for record in records]\n",
    "\n",
    "    cursor.executemany(insert_query, values)\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def connect_db_bulk_parallel(df):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunk_size = 5000  # Adjust the chunk size as needed\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system capacity\n",
    "        # Submit each chunk for parallel processing\n",
    "        futures = [executor.submit(connect_db_bulk, chunk,'monthly') for chunk in chunks]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def list_cs_files(bucket_name,prefix,delimiter=None): \n",
    "    storage_client = storage.Client()\n",
    "    data_dict = {\"daily\": [], \"monthly\": [], \"weekly\": []}\n",
    "    file_list = storage_client.list_blobs(bucket_name,prefix=prefix,delimiter=delimiter)\n",
    "    file_list = [file.name for file in file_list]\n",
    "    new_list = [s.split('/')[-1] for s in file_list]\n",
    "    monthly_files = [file for file in new_list if 'monthly' in file]\n",
    "    for file in monthly_files:\n",
    "        gcs_path = f'gs://{bucket_name}/{prefix}/{file}'  \n",
    "        company = file.split('_')[0]\n",
    "        data_dict[\"monthly\"].append(pd.read_csv(gcs_path).assign(company=company))\n",
    "    \n",
    "    monthly_data = pd.concat(data_dict[\"monthly\"], ignore_index=True)\n",
    "    # daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "    # daily_data['Volumn'] = daily_data['Volume'].astype('Float64')\n",
    "    column_mapping = {\n",
    "    'Date': 'date',\n",
    "    'Open': 'open',\n",
    "    'High': 'high',\n",
    "    'Low': 'low',\n",
    "    'Close': 'close',\n",
    "    'Adj Close': 'adj_close',\n",
    "    'Volume': 'volumn',\n",
    "    'Company': 'company'\n",
    "    }\n",
    "    # Map DataFrame columns to PostgreSQL columns\n",
    "    df_monthly_mapped = monthly_data.rename(columns=column_mapping)\n",
    "    connect_db_bulk_parallel(df_monthly_mapped)\n",
    "    return df_monthly_mapped.head()\n",
    "\n",
    "print(list_cs_files('dwdi-de-project','datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      open      high       low     close  adj_close      volumn  \\\n",
      "0  1997-05-12  0.121875  0.125000  0.085417  0.086458   0.086458  1737120000   \n",
      "1  1997-05-19  0.088021  0.088542  0.065625  0.075000   0.075000  1162824000   \n",
      "2  1997-05-26  0.075521  0.082292  0.072917  0.075000   0.075000   386784000   \n",
      "3  1997-06-02  0.075521  0.085417  0.068750  0.082813   0.082813   366696000   \n",
      "4  1997-06-09  0.082813  0.085417  0.076563  0.079167   0.079167   226488000   \n",
      "\n",
      "  company  \n",
      "0  AMAZON  \n",
      "1  AMAZON  \n",
      "2  AMAZON  \n",
      "3  AMAZON  \n",
      "4  AMAZON  \n"
     ]
    }
   ],
   "source": [
    "# Weekly Data Ingestion\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "register_adapter(np.int64, AsIs)\n",
    "\n",
    "# set key credentials file path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:/Disk_D/Course Work/Data Warehousing/Project-2/Keys/alien-grove-405422-bb57fda72219.json'\n",
    "\n",
    "def connect_db_bulk_parallel(df):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunk_size = 5000  # Adjust the chunk size as needed\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system capacity\n",
    "        # Submit each chunk for parallel processing\n",
    "        futures = [executor.submit(connect_db_bulk, chunk,'weekly') for chunk in chunks]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def list_cs_files(bucket_name,prefix,delimiter=None): \n",
    "    storage_client = storage.Client()\n",
    "    data_dict = {\"daily\": [], \"monthly\": [], \"weekly\": []}\n",
    "    file_list = storage_client.list_blobs(bucket_name,prefix=prefix,delimiter=delimiter)\n",
    "    file_list = [file.name for file in file_list]\n",
    "    new_list = [s.split('/')[-1] for s in file_list]\n",
    "    weekly_files = [file for file in new_list if 'weekly' in file]\n",
    "    for file in weekly_files:\n",
    "        gcs_path = f'gs://{bucket_name}/{prefix}/{file}'  \n",
    "        company = file.split('_')[0]\n",
    "        data_dict[\"weekly\"].append(pd.read_csv(gcs_path).assign(company=company))\n",
    "    \n",
    "    weekly_data = pd.concat(data_dict[\"weekly\"], ignore_index=True)\n",
    "    column_mapping = {\n",
    "    'Date': 'date',\n",
    "    'Open': 'open',\n",
    "    'High': 'high',\n",
    "    'Low': 'low',\n",
    "    'Close': 'close',\n",
    "    'Adj Close': 'adj_close',\n",
    "    'Volume': 'volumn',\n",
    "    'Company': 'company'\n",
    "    }\n",
    "    # Map DataFrame columns to PostgreSQL columns\n",
    "    df_weekly_mapped = weekly_data.rename(columns=column_mapping)\n",
    "    columns = ['open','high','low','close','adj_close']\n",
    "    connect_db_bulk_parallel(df_weekly_mapped)\n",
    "    return df_weekly_mapped.head()\n",
    "\n",
    "print(list_cs_files('dwdi-de-project','datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      open      high       low     close  adj_close      volumn  \\\n",
      "0  1997-05-15  0.121875  0.125000  0.096354  0.097917   0.097917  1443120000   \n",
      "1  1997-05-16  0.098438  0.098958  0.085417  0.086458   0.086458   294000000   \n",
      "2  1997-05-19  0.088021  0.088542  0.081250  0.085417   0.085417   122136000   \n",
      "3  1997-05-20  0.086458  0.087500  0.081771  0.081771   0.081771   109344000   \n",
      "4  1997-05-21  0.081771  0.082292  0.068750  0.071354   0.071354   377064000   \n",
      "\n",
      "  company  \n",
      "0  AMAZON  \n",
      "1  AMAZON  \n",
      "2  AMAZON  \n",
      "3  AMAZON  \n",
      "4  AMAZON  \n"
     ]
    }
   ],
   "source": [
    "# Daily Data Ingestion\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "register_adapter(np.int64, AsIs)\n",
    "\n",
    "\n",
    "# set key credentials file path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:/Disk_D/Course Work/Data Warehousing/Project-2/Keys/alien-grove-405422-bb57fda72219.json'\n",
    "\n",
    "def connect_db_bulk(df,tbname):\n",
    "    # Access database details\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "    # Use the variables in your database connection logic\n",
    "    connection_params = {\n",
    "        'host': db_host,\n",
    "        'port': db_port,\n",
    "        'database': db_name,\n",
    "        'user': db_user,\n",
    "        'password': db_password,\n",
    "    }\n",
    "\n",
    "    # Establish a connection to your PostgreSQL database\n",
    "    connection = psycopg2.connect(**connection_params)\n",
    "\n",
    "    # Create a cursor to execute SQL statements\n",
    "    cursor = connection.cursor()\n",
    "    table_name = tbname\n",
    "    schema_name = 'public'\n",
    "\n",
    "    insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "    records = df.to_records(index=False)\n",
    "    values = [tuple(record) for record in records]\n",
    "\n",
    "    cursor.executemany(insert_query, values)\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def connect_db_bulk_parallel(df):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunk_size = 5000  # Adjust the chunk size as needed\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:  # Adjust max_workers based on your system capacity\n",
    "        # Submit each chunk for parallel processing\n",
    "        futures = [executor.submit(connect_db_bulk, chunk,'daily') for chunk in chunks]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def list_cs_files(bucket_name,prefix,delimiter=None): \n",
    "    storage_client = storage.Client()\n",
    "    data_dict = {\"daily\": [], \"monthly\": [], \"weekly\": []}\n",
    "    file_list = storage_client.list_blobs(bucket_name,prefix=prefix,delimiter=delimiter)\n",
    "    file_list = [file.name for file in file_list]\n",
    "    new_list = [s.split('/')[-1] for s in file_list]\n",
    "    daily_files = [file for file in new_list if 'daily' in file]\n",
    "    for file in daily_files:\n",
    "        gcs_path = f'gs://{bucket_name}/{prefix}/{file}'  \n",
    "        company = file.split('_')[0]\n",
    "        data_dict[\"daily\"].append(pd.read_csv(gcs_path).assign(company=company))\n",
    "    \n",
    "    daily_data = pd.concat(data_dict[\"daily\"], ignore_index=True)\n",
    "    column_mapping = {\n",
    "    'Date': 'date',\n",
    "    'Open': 'open',\n",
    "    'High': 'high',\n",
    "    'Low': 'low',\n",
    "    'Close': 'close',\n",
    "    'Adj Close': 'adj_close',\n",
    "    'Volume': 'volumn',\n",
    "    'Company': 'company'\n",
    "    }\n",
    "    # Map DataFrame columns to PostgreSQL columns\n",
    "    df_daily_mapped = daily_data.rename(columns=column_mapping)\n",
    "    connect_db_bulk_parallel(df_daily_mapped)\n",
    "    return df_daily_mapped.head()\n",
    "\n",
    "print(list_cs_files('dwdi-de-project','datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1512104705.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      open      high       low     close  adj_close      volumn  \\\n",
      "0  1997-06-01  0.075521  0.085417  0.068750  0.077083   0.077083  1063272000   \n",
      "1  1997-07-01  0.077083  0.128646  0.075521  0.119792   0.119792  2172288000   \n",
      "2  1997-08-01  0.117188  0.120833  0.096875  0.116927   0.116927   638136000   \n",
      "3  1997-09-01  0.117188  0.240625  0.115625  0.216927   0.216927  2157240000   \n",
      "4  1997-10-01  0.221875  0.275000  0.176042  0.254167   0.254167  2103744000   \n",
      "\n",
      "  company  \n",
      "0  AMAZON  \n",
      "1  AMAZON  \n",
      "2  AMAZON  \n",
      "3  AMAZON  \n",
      "4  AMAZON  \n"
     ]
    }
   ],
   "source": [
    "# Monthly Data Ingestion check if data is already exists in the database\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "register_adapter(np.int64, AsIs)\n",
    "\n",
    "# set key credentials file path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:/Disk_D/Course Work/Data Warehousing/Project-2/Keys/alien-grove-405422-bb57fda72219.json'\n",
    "\n",
    "def connect_db_bulk(df,tbname):\n",
    "    #Get Credentials\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "    # Use the variables in your database connection logic\n",
    "    connection_params = {\n",
    "        'host': db_host,\n",
    "        'port': db_port,\n",
    "        'database': db_name,\n",
    "        'user': db_user,\n",
    "        'password': db_password,\n",
    "    }\n",
    "    # Establish a connection to your PostgreSQL database\n",
    "    connection = psycopg2.connect(**connection_params)\n",
    "\n",
    "    # Create a cursor to execute SQL statements\n",
    "    cursor = connection.cursor()\n",
    "    table_name = tbname\n",
    "    schema_name = 'public'\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    max_dates_by_company = df.groupby('company')['date'].max()\n",
    "    # max_dates_by_company_df = max_dates_by_company.reset_index()\n",
    "    max_dates_by_company = max_dates_by_company.reset_index().sort_values(by='company')\n",
    "    sql_query = \"SELECT company, MAX(date) as date FROM monthly GROUP BY company ORDER BY company;\"\n",
    "    # Execute the query and store the result in a DataFrame\n",
    "    db_result = pd.read_sql_query(sql_query, connection)\n",
    "    non_existence_check = ~df[\n",
    "        df[['date', 'company']].apply(\n",
    "            lambda row: (row['date'], row['company']) in db_result[['date', 'company']].values.tolist(),\n",
    "            axis=1\n",
    "        )\n",
    "    ]\n",
    "    non_existence_check_df = pd.DataFrame(non_existence_check, columns=df.columns)\n",
    "    if not non_existence_check_df.empty:\n",
    "        updated_df = pd.DataFrame()\n",
    "        updated_df = updated_df.assign(result = df.date.isin(non_existence_check_df.date))\n",
    "        insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "        records = updated_df.to_records(index=False)\n",
    "        values = [tuple(record) for record in records]\n",
    "        cursor.executemany(insert_query, values)\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def connect_db_bulk_parallel(df):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunk_size = 5000  # Adjust the chunk size as needed\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system capacity\n",
    "        # Submit each chunk for parallel processing\n",
    "        futures = [executor.submit(connect_db_bulk, chunk,'monthly') for chunk in chunks]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def list_cs_files(bucket_name,prefix,delimiter=None): \n",
    "    storage_client = storage.Client()\n",
    "    data_dict = {\"daily\": [], \"monthly\": [], \"weekly\": []}\n",
    "    file_list = storage_client.list_blobs(bucket_name,prefix=prefix,delimiter=delimiter)\n",
    "    file_list = [file.name for file in file_list]\n",
    "    new_list = [s.split('/')[-1] for s in file_list]\n",
    "    monthly_files = [file for file in new_list if 'monthly' in file]\n",
    "    for file in monthly_files:\n",
    "        gcs_path = f'gs://{bucket_name}/{prefix}/{file}'  \n",
    "        company = file.split('_')[0]\n",
    "        data_dict[\"monthly\"].append(pd.read_csv(gcs_path).assign(company=company))\n",
    "    \n",
    "    monthly_data = pd.concat(data_dict[\"monthly\"], ignore_index=True)\n",
    "    # daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "    # daily_data['Volumn'] = daily_data['Volume'].astype('Float64')\n",
    "    column_mapping = {\n",
    "    'Date': 'date',\n",
    "    'Open': 'open',\n",
    "    'High': 'high',\n",
    "    'Low': 'low',\n",
    "    'Close': 'close',\n",
    "    'Adj Close': 'adj_close',\n",
    "    'Volume': 'volumn',\n",
    "    'Company': 'company'\n",
    "    }\n",
    "    # Map DataFrame columns to PostgreSQL columns\n",
    "    df_monthly_mapped = monthly_data.rename(columns=column_mapping)\n",
    "    connect_db_bulk_parallel(df_monthly_mapped)\n",
    "    return df_monthly_mapped.head()\n",
    "\n",
    "print(list_cs_files('dwdi-de-project','datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\526782287.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\526782287.py:43: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\526782287.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\526782287.py:43: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      open      high       low     close  adj_close      volumn  \\\n",
      "0  1997-05-12  0.121875  0.125000  0.085417  0.086458   0.086458  1737120000   \n",
      "1  1997-05-19  0.088021  0.088542  0.065625  0.075000   0.075000  1162824000   \n",
      "2  1997-05-26  0.075521  0.082292  0.072917  0.075000   0.075000   386784000   \n",
      "3  1997-06-02  0.075521  0.085417  0.068750  0.082813   0.082813   366696000   \n",
      "4  1997-06-09  0.082813  0.085417  0.076563  0.079167   0.079167   226488000   \n",
      "\n",
      "  company  \n",
      "0  AMAZON  \n",
      "1  AMAZON  \n",
      "2  AMAZON  \n",
      "3  AMAZON  \n",
      "4  AMAZON  \n"
     ]
    }
   ],
   "source": [
    "# Weekly Data Ingestion check if data is already exists in the database\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "register_adapter(np.int64, AsIs)\n",
    "\n",
    "# set key credentials file path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:/Disk_D/Course Work/Data Warehousing/Project-2/Keys/alien-grove-405422-bb57fda72219.json'\n",
    "\n",
    "def connect_db_bulk(df,tbname):\n",
    "    #Get Credentials\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "    # Use the variables in your database connection logic\n",
    "    connection_params = {\n",
    "        'host': db_host,\n",
    "        'port': db_port,\n",
    "        'database': db_name,\n",
    "        'user': db_user,\n",
    "        'password': db_password,\n",
    "    }\n",
    "    # Establish a connection to your PostgreSQL database\n",
    "    connection = psycopg2.connect(**connection_params)\n",
    "\n",
    "    # Create a cursor to execute SQL statements\n",
    "    cursor = connection.cursor()\n",
    "    table_name = tbname\n",
    "    schema_name = 'public'\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    max_dates_by_company = df.groupby('company')['date'].max()\n",
    "    # max_dates_by_company_df = max_dates_by_company.reset_index()\n",
    "    max_dates_by_company = max_dates_by_company.reset_index().sort_values(by='company')\n",
    "    sql_query = \"SELECT company, MAX(date) as date FROM weekly GROUP BY company ORDER BY company;\"\n",
    "    # Execute the query and store the result in a DataFrame\n",
    "    db_result = pd.read_sql_query(sql_query, connection)\n",
    "    non_existence_check = ~df[\n",
    "        df[['date', 'company']].apply(\n",
    "            lambda row: (row['date'], row['company']) in db_result[['date', 'company']].values.tolist(),\n",
    "            axis=1\n",
    "        )\n",
    "    ]\n",
    "    non_existence_check_df = pd.DataFrame(non_existence_check, columns=df.columns)\n",
    "    if not non_existence_check_df.empty:\n",
    "        updated_df = pd.DataFrame()\n",
    "        updated_df = updated_df.assign(result = df.date.isin(non_existence_check_df.date))\n",
    "        insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "        records = updated_df.to_records(index=False)\n",
    "        values = [tuple(record) for record in records]\n",
    "        cursor.executemany(insert_query, values)\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def connect_db_bulk_parallel(df):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunk_size = 5000  # Adjust the chunk size as needed\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system capacity\n",
    "        # Submit each chunk for parallel processing\n",
    "        futures = [executor.submit(connect_db_bulk, chunk,'weekly') for chunk in chunks]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def list_cs_files(bucket_name,prefix,delimiter=None): \n",
    "    storage_client = storage.Client()\n",
    "    data_dict = {\"daily\": [], \"monthly\": [], \"weekly\": []}\n",
    "    file_list = storage_client.list_blobs(bucket_name,prefix=prefix,delimiter=delimiter)\n",
    "    file_list = [file.name for file in file_list]\n",
    "    new_list = [s.split('/')[-1] for s in file_list]\n",
    "    weekly_files = [file for file in new_list if 'weekly' in file]\n",
    "    for file in weekly_files:\n",
    "        gcs_path = f'gs://{bucket_name}/{prefix}/{file}'  \n",
    "        company = file.split('_')[0]\n",
    "        data_dict[\"weekly\"].append(pd.read_csv(gcs_path).assign(company=company))\n",
    "    \n",
    "    weekly_data = pd.concat(data_dict[\"weekly\"], ignore_index=True)\n",
    "    column_mapping = {\n",
    "    'Date': 'date',\n",
    "    'Open': 'open',\n",
    "    'High': 'high',\n",
    "    'Low': 'low',\n",
    "    'Close': 'close',\n",
    "    'Adj Close': 'adj_close',\n",
    "    'Volume': 'volumn',\n",
    "    'Company': 'company'\n",
    "    }\n",
    "    # Map DataFrame columns to PostgreSQL columns\n",
    "    df_weekly_mapped = weekly_data.rename(columns=column_mapping)\n",
    "    columns = ['open','high','low','close','adj_close']\n",
    "    connect_db_bulk_parallel(df_weekly_mapped)\n",
    "    return df_weekly_mapped.head()\n",
    "\n",
    "print(list_cs_files('dwdi-de-project','datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:50: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:50: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:50: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:50: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:50: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:50: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_26028\\1196511541.py:50: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      open      high       low     close  adj_close      volumn  \\\n",
      "0  1997-05-15  0.121875  0.125000  0.096354  0.097917   0.097917  1443120000   \n",
      "1  1997-05-16  0.098438  0.098958  0.085417  0.086458   0.086458   294000000   \n",
      "2  1997-05-19  0.088021  0.088542  0.081250  0.085417   0.085417   122136000   \n",
      "3  1997-05-20  0.086458  0.087500  0.081771  0.081771   0.081771   109344000   \n",
      "4  1997-05-21  0.081771  0.082292  0.068750  0.071354   0.071354   377064000   \n",
      "\n",
      "  company  \n",
      "0  AMAZON  \n",
      "1  AMAZON  \n",
      "2  AMAZON  \n",
      "3  AMAZON  \n",
      "4  AMAZON  \n"
     ]
    }
   ],
   "source": [
    "# Daily Data Ingestion if data exists in the database\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "register_adapter(np.int64, AsIs)\n",
    "\n",
    "\n",
    "# set key credentials file path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:/Disk_D/Course Work/Data Warehousing/Project-2/Keys/alien-grove-405422-bb57fda72219.json'\n",
    "\n",
    "def connect_db_bulk(df,tbname):\n",
    "    # Access database details\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "    # Use the variables in your database connection logic\n",
    "    connection_params = {\n",
    "        'host': db_host,\n",
    "        'port': db_port,\n",
    "        'database': db_name,\n",
    "        'user': db_user,\n",
    "        'password': db_password,\n",
    "    }\n",
    "\n",
    "    # Establish a connection to your PostgreSQL database\n",
    "    connection = psycopg2.connect(**connection_params)\n",
    "\n",
    "    # Create a cursor to execute SQL statements\n",
    "    cursor = connection.cursor()\n",
    "    table_name = tbname\n",
    "    schema_name = 'public'\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    max_dates_by_company = df.groupby('company')['date'].max()\n",
    "    # max_dates_by_company_df = max_dates_by_company.reset_index()\n",
    "    max_dates_by_company = max_dates_by_company.reset_index().sort_values(by='company')\n",
    "    sql_query = \"SELECT company, MAX(date) as date FROM daily GROUP BY company ORDER BY company;\"\n",
    "    # Execute the query and store the result in a DataFrame\n",
    "    db_result = pd.read_sql_query(sql_query, connection)\n",
    "    non_existence_check = ~df[\n",
    "        df[['date', 'company']].apply(\n",
    "            lambda row: (row['date'], row['company']) in db_result[['date', 'company']].values.tolist(),\n",
    "            axis=1\n",
    "        )\n",
    "    ]\n",
    "    non_existence_check_df = pd.DataFrame(non_existence_check, columns=df.columns)\n",
    "    if not non_existence_check_df.empty:\n",
    "        updated_df = pd.DataFrame()\n",
    "        updated_df = updated_df.assign(result = df.date.isin(non_existence_check_df.date))\n",
    "        insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "        records = updated_df.to_records(index=False)\n",
    "        values = [tuple(record) for record in records]\n",
    "        cursor.executemany(insert_query, values)\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def connect_db_bulk_parallel(df):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunk_size = 5000  # Adjust the chunk size as needed\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:  # Adjust max_workers based on your system capacity\n",
    "        # Submit each chunk for parallel processing\n",
    "        futures = [executor.submit(connect_db_bulk, chunk,'daily') for chunk in chunks]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def list_cs_files(bucket_name,prefix,delimiter=None): \n",
    "    storage_client = storage.Client()\n",
    "    data_dict = {\"daily\": [], \"monthly\": [], \"weekly\": []}\n",
    "    file_list = storage_client.list_blobs(bucket_name,prefix=prefix,delimiter=delimiter)\n",
    "    file_list = [file.name for file in file_list]\n",
    "    new_list = [s.split('/')[-1] for s in file_list]\n",
    "    daily_files = [file for file in new_list if 'daily' in file]\n",
    "    for file in daily_files:\n",
    "        gcs_path = f'gs://{bucket_name}/{prefix}/{file}'  \n",
    "        company = file.split('_')[0]\n",
    "        data_dict[\"daily\"].append(pd.read_csv(gcs_path).assign(company=company))\n",
    "    \n",
    "    daily_data = pd.concat(data_dict[\"daily\"], ignore_index=True)\n",
    "    column_mapping = {\n",
    "    'Date': 'date',\n",
    "    'Open': 'open',\n",
    "    'High': 'high',\n",
    "    'Low': 'low',\n",
    "    'Close': 'close',\n",
    "    'Adj Close': 'adj_close',\n",
    "    'Volume': 'volumn',\n",
    "    'Company': 'company'\n",
    "    }\n",
    "    # Map DataFrame columns to PostgreSQL columns\n",
    "    df_daily_mapped = daily_data.rename(columns=column_mapping)\n",
    "    connect_db_bulk_parallel(df_daily_mapped)\n",
    "    return df_daily_mapped.head()\n",
    "\n",
    "print(list_cs_files('dwdi-de-project','datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: \n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "         date      open      high       low     close  adj_close      volumn  \\\n",
      "0  1997-06-01  0.075521  0.085417  0.068750  0.077083   0.077083  1063272000   \n",
      "1  1997-07-01  0.077083  0.128646  0.075521  0.119792   0.119792  2172288000   \n",
      "2  1997-08-01  0.117188  0.120833  0.096875  0.116927   0.116927   638136000   \n",
      "3  1997-09-01  0.117188  0.240625  0.115625  0.216927   0.216927  2157240000   \n",
      "4  1997-10-01  0.221875  0.275000  0.176042  0.254167   0.254167  2103744000   \n",
      "\n",
      "  company  \n",
      "0  AMAZON  \n",
      "1  AMAZON  \n",
      "2  AMAZON  \n",
      "3  AMAZON  \n",
      "4  AMAZON  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\1448821119.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n"
     ]
    }
   ],
   "source": [
    "# Monthly Data Ingestion check if data is already exists in the database\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "register_adapter(np.int64, AsIs)\n",
    "\n",
    "# set key credentials file path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:/Disk_D/Course Work/Data Warehousing/Project-2/Keys/alien-grove-405422-bb57fda72219.json'\n",
    "\n",
    "def connect_db_bulk(df,tbname):\n",
    "    #Get Credentials\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "    # Use the variables in your database connection logic\n",
    "    connection_params = {\n",
    "        'host': db_host,\n",
    "        'port': db_port,\n",
    "        'database': db_name,\n",
    "        'user': db_user,\n",
    "        'password': db_password,\n",
    "    }\n",
    "    # Establish a connection to your PostgreSQL database\n",
    "    connection = psycopg2.connect(**connection_params)\n",
    "\n",
    "    # Create a cursor to execute SQL statements\n",
    "    cursor = connection.cursor()\n",
    "    table_name = tbname\n",
    "    schema_name = 'public'\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    max_dates_by_company = df.groupby('company')['date'].max()\n",
    "    # max_dates_by_company_df = max_dates_by_company.reset_index()\n",
    "    max_dates_by_company = max_dates_by_company.reset_index().sort_values(by='company')\n",
    "    # print(max_dates_by_company)\n",
    "    sql_query = \"SELECT company, MAX(date) as date FROM public.testmonthly GROUP BY company ORDER BY company;\"\n",
    "    # Execute the query and store the result in a DataFrame\n",
    "    db_result = pd.read_sql_query(sql_query, connection)\n",
    "    db_result['date'] = pd.to_datetime(db_result['date']).dt.date\n",
    "    res_cd = pd.DataFrame()\n",
    "    if len(db_result) == 0:\n",
    "        # res_cd = max_dates_by_company\n",
    "        insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "        records = df.to_records(index=False)\n",
    "        values = [tuple(record) for record in records]\n",
    "        cursor.executemany(insert_query, values)\n",
    "    else:\n",
    "        db_result['date'] = pd.to_datetime(db_result['date']).dt.date\n",
    "        for i in range(len(db_result)):\n",
    "            if db_result.loc[i, \"date\"] < max_dates_by_company.loc[i, \"date\"]:\n",
    "                res_cd = res_cd.append(db_result.loc[i])\n",
    "        # res_cd['date'] = pd.to_datetime(res_cd['date']).dt.date\n",
    "        res = pd.DataFrame()\n",
    "        for i in range(len(res_cd)):\n",
    "            company_res = df[(df['company'] == res_cd.loc[i, 'company']) & (df['date'] > res_cd.loc[i, 'date'])]\n",
    "            # print(\"company_res\",company_res)\n",
    "            res = res.append(company_res)\n",
    "        \n",
    "        print(\"result: \\n\",res)\n",
    "        if(len(res)!=0):\n",
    "            insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "            records = res.to_records(index=False)\n",
    "            values = [tuple(record) for record in records]\n",
    "            cursor.executemany(insert_query, values)\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def connect_db_bulk_parallel(df):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunk_size = 5000  # Adjust the chunk size as needed\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system capacity\n",
    "        # Submit each chunk for parallel processing\n",
    "        futures = [executor.submit(connect_db_bulk, chunk,'testmonthly') for chunk in chunks]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def list_cs_files(bucket_name,prefix,delimiter=None): \n",
    "    storage_client = storage.Client()\n",
    "    data_dict = {\"daily\": [], \"monthly\": [], \"weekly\": []}\n",
    "    file_list = storage_client.list_blobs(bucket_name,prefix=prefix,delimiter=delimiter)\n",
    "    file_list = [file.name for file in file_list]\n",
    "    new_list = [s.split('/')[-1] for s in file_list]\n",
    "    monthly_files = [file for file in new_list if 'monthly' in file]\n",
    "    for file in monthly_files:\n",
    "        gcs_path = f'gs://{bucket_name}/{prefix}/{file}'  \n",
    "        company = file.split('_')[0]\n",
    "        data_dict[\"monthly\"].append(pd.read_csv(gcs_path).assign(company=company))\n",
    "    \n",
    "    monthly_data = pd.concat(data_dict[\"monthly\"], ignore_index=True)\n",
    "    # daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "    # daily_data['Volumn'] = daily_data['Volume'].astype('Float64')\n",
    "    column_mapping = {\n",
    "    'Date': 'date',\n",
    "    'Open': 'open',\n",
    "    'High': 'high',\n",
    "    'Low': 'low',\n",
    "    'Close': 'close',\n",
    "    'Adj Close': 'adj_close',\n",
    "    'Volume': 'volumn',\n",
    "    'Company': 'company'\n",
    "    }\n",
    "    # Map DataFrame columns to PostgreSQL columns\n",
    "    df_monthly_mapped = monthly_data.rename(columns=column_mapping)\n",
    "    connect_db_bulk_parallel(df_monthly_mapped)\n",
    "    return df_monthly_mapped.head()\n",
    "\n",
    "print(list_cs_files('dwdi-de-project','datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\18572\\anaconda3\\lib\\site-packages (1.5.16)\n",
      "Requirement already satisfied: requests in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (2.28.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (1.26.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (2022.9.14)\n",
      "Requirement already satisfied: tqdm in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (4.64.1)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\18572\\anaconda3\\lib\\site-packages (from bleach->kaggle) (21.3)\n",
      "Requirement already satisfied: webencodings in c:\\users\\18572\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\18572\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from packaging->bleach->kaggle) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:39: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  db_result = pd.read_sql_query(sql_query, connection)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res_cd = res_cd.append(db_result.loc[i])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res_cd = res_cd.append(db_result.loc[i])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res_cd = res_cd.append(db_result.loc[i])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res_cd = res_cd.append(db_result.loc[i])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res_cd = res_cd.append(db_result.loc[i])\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res = res.append(company_res)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res = res.append(company_res)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res = res.append(company_res)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res = res.append(company_res)\n",
      "C:\\Users\\18572\\AppData\\Local\\Temp\\ipykernel_25672\\839482090.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res = res.append(company_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: \n",
      "             date        open        high         low       close   adj_close  \\\n",
      "316   2023-10-01  127.279999  134.479996  118.349998  133.089996  133.089996   \n",
      "317   2023-11-01  133.960007  149.259995  133.710007  147.729996  147.729996   \n",
      "783   2023-10-01  171.220001  182.339996  165.669998  170.770004  170.545319   \n",
      "784   2023-11-01  171.000000  192.929993  170.119995  189.789993  189.540283   \n",
      "1014  2023-10-01  132.154999  142.380005  121.459999  125.300003  125.300003   \n",
      "1015  2023-11-01  125.339996  141.100006  124.925003  138.050003  138.050003   \n",
      "1152  2023-10-01  302.739990  330.540009  279.399994  301.269989  301.269989   \n",
      "1153  2023-11-01  301.850006  342.920013  301.850006  334.700012  334.700012   \n",
      "1410  2023-10-01  377.480011  418.839996  344.730011  411.690002  411.690002   \n",
      "1411  2023-11-01  414.769989  482.700012  414.179993  479.170013  479.170013   \n",
      "\n",
      "          volumn  company  \n",
      "316   1224564700   AMAZON  \n",
      "317    876754600   AMAZON  \n",
      "783   1172719600    APPLE  \n",
      "784    969310000    APPLE  \n",
      "1014   514877100   GOOGLE  \n",
      "1015   337642400   GOOGLE  \n",
      "1152   511307900     META  \n",
      "1153   277424200     META  \n",
      "1410   164021900  NETFLIX  \n",
      "1411    61706300  NETFLIX  \n",
      "         date      open      high       low     close  adj_close      volumn  \\\n",
      "0  1997-06-01  0.075521  0.085417  0.068750  0.077083   0.077083  1063272000   \n",
      "1  1997-07-01  0.077083  0.128646  0.075521  0.119792   0.119792  2172288000   \n",
      "2  1997-08-01  0.117188  0.120833  0.096875  0.116927   0.116927   638136000   \n",
      "3  1997-09-01  0.117188  0.240625  0.115625  0.216927   0.216927  2157240000   \n",
      "4  1997-10-01  0.221875  0.275000  0.176042  0.254167   0.254167  2103744000   \n",
      "\n",
      "  company  \n",
      "0  AMAZON  \n",
      "1  AMAZON  \n",
      "2  AMAZON  \n",
      "3  AMAZON  \n",
      "4  AMAZON  \n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import shutil\n",
    "# Set your Kaggle API key\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.dataset_download_files(dataset=\"nikhil1e9/netflix-stock-price\", path=\"datasets/\", unzip=True)\n",
    "\n",
    "def connect_db_bulk(df,tbname):\n",
    "    #Get Credentials\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "    # Use the variables in your database connection logic\n",
    "    connection_params = {\n",
    "        'host': db_host,\n",
    "        'port': db_port,\n",
    "        'database': db_name,\n",
    "        'user': db_user,\n",
    "        'password': db_password,\n",
    "    }\n",
    "    # Establish a connection to your PostgreSQL database\n",
    "    connection = psycopg2.connect(**connection_params)\n",
    "\n",
    "    # Create a cursor to execute SQL statements\n",
    "    cursor = connection.cursor()\n",
    "    table_name = tbname\n",
    "    schema_name = 'public'\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    max_dates_by_company = df.groupby('company')['date'].max()\n",
    "    # max_dates_by_company_df = max_dates_by_company.reset_index()\n",
    "    max_dates_by_company = max_dates_by_company.reset_index().sort_values(by='company')\n",
    "    # print(max_dates_by_company)\n",
    "    sql_query = \"SELECT company, MAX(date) as date FROM public.testmonthly GROUP BY company ORDER BY company;\"\n",
    "    # Execute the query and store the result in a DataFrame\n",
    "    db_result = pd.read_sql_query(sql_query, connection)\n",
    "    db_result['date'] = pd.to_datetime(db_result['date']).dt.date\n",
    "    res_cd = pd.DataFrame()\n",
    "    if len(db_result) == 0:\n",
    "        # res_cd = max_dates_by_company\n",
    "        insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "        records = df.to_records(index=False)\n",
    "        values = [tuple(record) for record in records]\n",
    "        cursor.executemany(insert_query, values)\n",
    "    else:\n",
    "        db_result['date'] = pd.to_datetime(db_result['date']).dt.date\n",
    "        for i in range(len(db_result)):\n",
    "            if db_result.loc[i, \"date\"] < max_dates_by_company.loc[i, \"date\"]:\n",
    "                res_cd = res_cd.append(db_result.loc[i])\n",
    "        # res_cd['date'] = pd.to_datetime(res_cd['date']).dt.date\n",
    "        res = pd.DataFrame()\n",
    "        for i in range(len(res_cd)):\n",
    "            company_res = df[(df['company'] == res_cd.loc[i, 'company']) & (df['date'] > res_cd.loc[i, 'date'])]\n",
    "            # print(\"company_res\",company_res)\n",
    "            res = res.append(company_res)\n",
    "        \n",
    "        # print(\"result: \\n\",res)\n",
    "        if(len(res)!=0):\n",
    "            insert_query = f\"INSERT INTO {schema_name}.{table_name} VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "            records = res.to_records(index=False)\n",
    "            values = [tuple(record) for record in records]\n",
    "            cursor.executemany(insert_query, values)\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def connect_db_bulk_parallel(df):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunk_size = 5000  # Adjust the chunk size as needed\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system capacity\n",
    "        # Submit each chunk for parallel processing\n",
    "        futures = [executor.submit(connect_db_bulk, chunk,'testmonthly') for chunk in chunks]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def list_csv_files(): \n",
    "    downloaded_files = os.listdir('datasets')\n",
    "    data_dict = {\"daily\": [], \"monthly\": [], \"weekly\": []}\n",
    "    file_list = [file for file in downloaded_files]\n",
    "    new_list = [s.split('/')[-1] for s in file_list]\n",
    "    monthly_files = [file for file in new_list if 'monthly' in file]\n",
    "    for file in monthly_files:\n",
    "        gcs_path = 'datasets/'  \n",
    "        company = file.split('_')[0]\n",
    "        data_dict[\"monthly\"].append(pd.read_csv(gcs_path+file).assign(company=company))\n",
    "    monthly_data = pd.concat(data_dict[\"monthly\"], ignore_index=True)\n",
    "\n",
    "    # for file_name in file_list:\n",
    "    #     file_path = os.path.join(os.listdir('datasets'), file_name)\n",
    "    directory_path = 'datasets'\n",
    "    shutil.rmtree(directory_path)\n",
    "    \n",
    "    column_mapping = {\n",
    "    'Date': 'date',\n",
    "    'Open': 'open',\n",
    "    'High': 'high',\n",
    "    'Low': 'low',\n",
    "    'Close': 'close',\n",
    "    'Adj Close': 'adj_close',\n",
    "    'Volume': 'volumn',\n",
    "    'Company': 'company'\n",
    "    }\n",
    "    # Map DataFrame columns to PostgreSQL columns\n",
    "    df_monthly_mapped = monthly_data.rename(columns=column_mapping)\n",
    "    connect_db_bulk_parallel(df_monthly_mapped)\n",
    "    return df_monthly_mapped.head()\n",
    "\n",
    "print(list_csv_files())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\18572\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.1.3-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "     --------------------------------------- 10.8/10.8 MB 24.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ------------------------------------- 341.8/341.8 kB 20.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\18572\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\18572\\\\anaconda3\\\\Lib\\\\site-packages\\\\~andas\\\\_libs\\\\algos.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
